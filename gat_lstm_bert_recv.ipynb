{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11376620,"sourceType":"datasetVersion","datasetId":7122612},{"sourceId":11376656,"sourceType":"datasetVersion","datasetId":7122642},{"sourceId":11404051,"sourceType":"datasetVersion","datasetId":7142902},{"sourceId":11405931,"sourceType":"datasetVersion","datasetId":7132411},{"sourceId":233104996,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom transformers import BertTokenizer\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.metrics import f1_score\nimport nltk\nnltk.download('punkt_tab')\n\nfrom tqdm import tqdm\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c43KEZVWTPYQ","outputId":"b6793a8d-127e-415e-e37d-c4404ec28581","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:51:47.649358Z","iopub.execute_input":"2025-04-14T16:51:47.649558Z","iopub.status.idle":"2025-04-14T16:51:58.039144Z","shell.execute_reply.started":"2025-04-14T16:51:47.649541Z","shell.execute_reply":"2025-04-14T16:51:58.038423Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:10.592481Z","iopub.execute_input":"2025-04-14T16:52:10.593279Z","iopub.status.idle":"2025-04-14T16:52:16.050064Z","shell.execute_reply.started":"2025-04-14T16:52:10.593252Z","shell.execute_reply":"2025-04-14T16:52:16.049334Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.16)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import random\n\nseed = 100\n\ntorch.manual_seed(seed)\nrandom.seed(seed)\ntorch.cuda.manual_seed_all(seed)\nrandom.seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"id":"xcQyz2yhVv5W","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:16.051853Z","iopub.execute_input":"2025-04-14T16:52:16.052164Z","iopub.status.idle":"2025-04-14T16:52:16.062371Z","shell.execute_reply.started":"2025-04-14T16:52:16.052142Z","shell.execute_reply":"2025-04-14T16:52:16.061597Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"with open(\"/kaggle/input/diplomacy-dataset/test.jsonl\", \"r\", encoding=\"utf-8\") as file:\n    test_data = [json.loads(line) for line in file]\n\nwith open(\"/kaggle/input/diplomacy-dataset/train.jsonl\", \"r\", encoding=\"utf-8\") as file:\n    train_data = [json.loads(line) for line in file]\n\nwith open(\"/kaggle/input/diplomacy-dataset/validation.jsonl\", \"r\", encoding=\"utf-8\") as file:\n    val_data = [json.loads(line) for line in file]\n\n","metadata":{"id":"xdVAb-FnTPYR","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:16.063183Z","iopub.execute_input":"2025-04-14T16:52:16.063709Z","iopub.status.idle":"2025-04-14T16:52:16.173617Z","shell.execute_reply.started":"2025-04-14T16:52:16.063683Z","shell.execute_reply":"2025-04-14T16:52:16.172850Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def preprocess(sentence ):\n    sentence=sentence.lower()\n\n    sentence = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", sentence)  # can use punctations with bert , not with glove\n    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n\n    return sentence\n\ndef prep_data_context(data ,  is_sender ):\n    final_data=[]\n    for data_points in data:\n        sub = []\n        for i, message in enumerate(data_points[\"messages\"]):\n\n            msg=preprocess(message )\n            msg=word_tokenize(msg)\n            if(len(msg)==0): continue\n\n            if(is_sender):\n              if(data_points['sender_labels'][i]=='NOANNOTATION'):\n                continue\n            else:\n              if(data_points['receiver_labels'][i]=='NOANNOTATION'):\n                continue\n\n            sub.append({\"message\":msg ,\n                        \"label\":(data_points[\"receiver_labels\"][i],data_points[\"sender_labels\"][i] )[is_sender] ,\n                        \"game_score_delta\": int(data_points[\"game_score_delta\"][i])})\n        if(len(sub)==0): continue\n        final_data.append(sub)\n    return final_data","metadata":{"id":"8qal_iBqTPYS","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:16.175038Z","iopub.execute_input":"2025-04-14T16:52:16.175303Z","iopub.status.idle":"2025-04-14T16:52:16.181499Z","shell.execute_reply.started":"2025-04-14T16:52:16.175280Z","shell.execute_reply":"2025-04-14T16:52:16.180976Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"val=prep_data_context(val_data, 0)\ntrain=prep_data_context(train_data,0)\ntest=prep_data_context(test_data ,0)","metadata":{"id":"-1sXWsKaTPYS","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:17.389132Z","iopub.execute_input":"2025-04-14T16:52:17.389758Z","iopub.status.idle":"2025-04-14T16:52:19.224976Z","shell.execute_reply.started":"2025-04-14T16:52:17.389723Z","shell.execute_reply":"2025-04-14T16:52:19.224403Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# tokens = []\n# for sub in train:\n#   for data_p in sub:\n#     for word in data_p[\"message\"]:\n#         tokens.append(word)\n# tokens=sorted(set(tokens))\n\n# vocab = {token:idx+2  for idx , token in enumerate(tokens)}\n# vocab[\"<PAD>\"]=0\n# vocab[\"<UNK>\"]=1\nfrom transformers import BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"id":"Wsx9mhKOTPYS","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:19.226135Z","iopub.execute_input":"2025-04-14T16:52:19.226416Z","iopub.status.idle":"2025-04-14T16:52:43.251223Z","shell.execute_reply.started":"2025-04-14T16:52:19.226391Z","shell.execute_reply":"2025-04-14T16:52:43.250646Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 16:52:26.233199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744649546.515284      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744649546.586122      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"297efbc4115841738fbf7c060d1388c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df73b5daae934246bddd192aabff6fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e84f2393d8c4c128b105f83060914ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fafbf84036a343e39d1766dcea208130"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class Deception_dataset_context(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        data_p = self.data[idx]\n        msg_ids = []\n        for sub in data_p:\n            # Join the pre-tokenized words into a single string.\n            # The BertTokenizer will perform its own tokenization.\n            text = \" \".join(sub['message'])\n            # Get token IDs from BERT tokenizer.\n            token_ids = self.tokenizer.encode(text, add_special_tokens=False)\n            msg_ids.append(torch.tensor(token_ids, dtype=torch.long))\n        try:\n            return {\n                \"messages\": msg_ids,\n                \"labels\": torch.tensor([i['label'] for i in data_p], dtype=torch.long),\n                \"game_score_delta\": torch.tensor([i[\"game_score_delta\"] for i in data_p], dtype=torch.float)\n            }\n        except Exception as e:\n            print(\"issue: \", e)\n            return\n","metadata":{"id":"XZs0LKaJcRwl","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:43.251916Z","iopub.execute_input":"2025-04-14T16:52:43.252393Z","iopub.status.idle":"2025-04-14T16:52:43.258437Z","shell.execute_reply.started":"2025-04-14T16:52:43.252368Z","shell.execute_reply":"2025-04-14T16:52:43.257584Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def collate_fn_context(batch):\n    messages = []\n    labels = []\n    lengths = []\n    num_messages = []\n    game_score_deltas = []\n    for i in batch:\n        messages.extend(i['messages'])\n        lengths.extend([j.shape[0] for j in i['messages']])\n        labels.extend(i['labels'])\n        num_messages.append(len(i['messages']))\n        game_score_deltas.extend(i['game_score_delta'])\n    padded_messages = pad_sequence(messages, batch_first=True, padding_value=0)\n    return {\n        \"messages\": padded_messages,\n        \"lengths\": lengths,\n        \"labels\": torch.tensor(labels),\n        \"num_messages\": num_messages,\n        \"deltas\":torch.tensor(game_score_deltas)\n    }\n","metadata":{"id":"PE2fGpjJetfL","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:43.260000Z","iopub.execute_input":"2025-04-14T16:52:43.260766Z","iopub.status.idle":"2025-04-14T16:52:43.468405Z","shell.execute_reply.started":"2025-04-14T16:52:43.260734Z","shell.execute_reply":"2025-04-14T16:52:43.467605Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# def load_glove(file):\n#     embeddings = {}\n#     with open(file, 'r', encoding='utf-8') as f:\n#         for line in f:\n#             embed = line.split()\n#             word = embed[0]\n#             embedding = torch.tensor([float(i) for i in embed[1:]], dtype=torch.float)\n#             embeddings[word] = embedding\n#     return embeddings","metadata":{"id":"euYr7GTFTPYT","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:43.469306Z","iopub.execute_input":"2025-04-14T16:52:43.469640Z","iopub.status.idle":"2025-04-14T16:52:43.482069Z","shell.execute_reply.started":"2025-04-14T16:52:43.469622Z","shell.execute_reply":"2025-04-14T16:52:43.481465Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import random\nimport torch_geometric\nfrom torch_geometric.data import Batch, Data\n\nclass ContextLSTM(nn.Module):\n  def __init__(self, embedding_dim, hidden_size_message, gat_dim, num_classes):\n    super(ContextLSTM,self).__init__()\n    # glove_embeddings = load_glove(glove_file)\n\n    # vocab_size = len(vocab)\n    # self.embedding_matrix = torch.zeros(vocab_size, embedding_dim)\n    # for token, idx in vocab.items():\n    #     if token in glove_embeddings:\n    #         self.embedding_matrix[idx] = glove_embeddings[token]\n    #     else:\n    #         self.embedding_matrix[idx] = torch.randn(embedding_dim) * 0.6\n\n    # self.embedding = nn.Embedding.from_pretrained(self.embedding_matrix, freeze=True, padding_idx=0)\n    self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n    self.embedding = self.bert_model.embeddings.word_embeddings\n    # Unfreeze BERT embeddings so they can be fine-tuned\n    for param in self.bert_model.embeddings.parameters():\n        param.requires_grad = False\n\n    self.lstm_message = nn.LSTM(embedding_dim, hidden_size_message, batch_first=True, bidirectional=True)\n    # self.gat1 = torch_geometric.nn.conv.GATConv(in_channels=hidden_size_message*2,out_channels=gat_dim//2,heads=4)\n    self.gat1 = torch_geometric.nn.conv.GATConv(in_channels=hidden_size_message*2,out_channels=gat_dim*2,heads=4,  concat=False)\n      \n    self.gat2 = torch_geometric.nn.conv.GATConv(in_channels=gat_dim*2,out_channels=gat_dim,heads=2 , concat=False)\n    self.fc = nn.Linear(hidden_size_message * 2 + gat_dim, num_classes)\n    self.relu = torch.nn.ReLU()\n    self.tanh = torch.nn.Tanh()\n    self.sigmoid = torch.nn.Sigmoid()  \n    self.gate1 = nn.Linear(hidden_size_message * 2 + gat_dim,hidden_size_message * 2)\n    self.gate2 = nn.Linear(hidden_size_message * 2 + gat_dim,gat_dim)\n    self.hidden_size_message = hidden_size_message\n    self.gat_dim = gat_dim\n\n  def fusion(self,x):\n      lstm_emb = x[:,:2*self.hidden_size_message]\n      gat_emb = x[:,2*self.hidden_size_message:]\n      \n      lstm_emb_tanh = self.tanh(lstm_emb) \n      gat_emb_tanh = self.tanh(gat_emb)\n      # print('lstm emb: ',lstm_emb.shape)\n      # print(\"gat emb shape: \",gat_emb.shape)\n      mixed = torch.cat((lstm_emb,gat_emb),dim=1)\n      mixed_sigmoid_left = self.sigmoid(self.gate1(mixed))\n      mixed_sigmoid_right = self.sigmoid(self.gate2(mixed))\n\n      final_emb = torch.cat((mixed_sigmoid_left*lstm_emb_tanh,mixed_sigmoid_right*gat_emb_tanh),dim=1)\n      return final_emb\n\n  def get_message_emb(self,input_ids,lengths):\n    embedded = self.embedding(input_ids)\n    # print(embedded.shape)\n    packed = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n\n    _, (hn, _1) = self.lstm_message(packed)\n    last_hidden = torch.cat((hn[0], hn[1]), dim=1)\n    return last_hidden\n\n  \n\n  def forward(self, input_ids, num_messages, lengths,scores):\n    total_messages = sum(num_messages)\n    inputs_to_msg_encoder = input_ids\n    encoded_messages = self.get_message_emb(inputs_to_msg_encoder,lengths)\n    input_to_convo_encoder = torch.split(encoded_messages,num_messages)\n    # print(input_to_convo_encoder[0].shape)\n\n    tot_data = []\n\n    for i in range(len(input_to_convo_encoder)):\n        msg_embs = input_to_convo_encoder[i] # num_msgs x emb_dim\n        n_nodes = msg_embs.shape[0]\n        edge_ind = torch.combinations(torch.arange(n_nodes), r=2).T\n        edge_ind = torch.cat([edge_ind, edge_ind.flip(0)], dim=1).to(input_ids.device)\n    \n        tot_data.append(Data(x=msg_embs,edge_index=edge_ind))\n\n    tot_data = Batch.from_data_list(tot_data)\n    from_gat1 = self.relu(self.gat1(tot_data.x,tot_data.edge_index))\n    \n    # print(\"from gat 1 shape: \",from_gat1.shape)\n\n    from_gat2 = self.gat2(from_gat1,tot_data.edge_index)\n    # print(\"enc msg shape: \",encoded_messages.shape)\n    # print(\"from_gat2 shap: \",from_gat2.shape)\n      \n    input_for_fusion = [] \n    prev = 0\n    # print(\"enc msg shape: \",encoded_messages.shape)\n    for i, msg_emb in enumerate(encoded_messages):\n        # print(\"i: \",i)\n        inp = torch.cat((msg_emb,from_gat2[i]),dim=0)\n        input_for_fusion.append(inp)\n    # print(\"shape before fusion: \",torch.stack(input_for_fusion).shape)\n    final_combined = self.fusion(torch.stack(input_for_fusion))\n    # print(\"final combined shape: \",final_combined.shape)\n    final_input = torch.cat((final_combined,  scores.unsqueeze(1)), dim=1)\n    # print(final_input.shape)\n    logits = self.fc(final_combined)\n    # print(\"logits shape: \",logits.shape)\n    return logits\n\n","metadata":{"id":"hG6f4m63ND-N","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:52:43.482723Z","iopub.execute_input":"2025-04-14T16:52:43.482977Z","iopub.status.idle":"2025-04-14T16:52:45.041187Z","shell.execute_reply.started":"2025-04-14T16:52:43.482955Z","shell.execute_reply":"2025-04-14T16:52:45.040616Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Train Context LSTM:","metadata":{"id":"HW04PFmPgQoD"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfrom sklearn.metrics import accuracy_score\n\n# Set embedding_dim to 768 for BERT embeddings.\nembedding_dim = 768\nhidden_size = 128\nnum_classes = 2\ngat_dim=50\n\nseed = 100\n\ntorch.manual_seed(seed)\nrandom.seed(seed)\ntorch.cuda.manual_seed_all(seed)\nrandom.seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nmodel = ContextLSTM(embedding_dim, hidden_size, gat_dim, num_classes)\nmodel = model.to(device)\n\nclass_weights = torch.tensor([1.0 / 0.05, 1.0 / 0.95], dtype=torch.float).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\nloss = nn.CrossEntropyLoss(weight=class_weights)\n\ntrain_dataset = Deception_dataset_context(train, tokenizer)\nval_dataset = Deception_dataset_context(val, tokenizer)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_context)\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_context)\ntest_dataset = Deception_dataset_context(test, tokenizer)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_context)\n\nfor epoch in range(15):\n    model.train()\n    train_loss = 0\n    train_preds, train_labels = [], []\n    for batch in train_dataloader:\n        messages = batch[\"messages\"].to(device)\n        lengths = batch[\"lengths\"]\n        labels = batch[\"labels\"].to(device)\n        num_messages = batch['num_messages']\n        scores = batch['deltas'].to(device)\n        optimizer.zero_grad()\n        logits = model(messages, num_messages, lengths,scores)\n        loss_ = loss(logits, labels)\n        loss_.backward()\n        optimizer.step()\n\n        train_loss += loss_.item()\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n        train_preds.extend(preds)\n        train_labels.extend(labels.cpu().numpy())\n\n    train_loss /= len(train_dataloader)\n    train_f1 = f1_score(train_labels, train_preds, average='macro')\n    train_acc = accuracy_score(train_labels,train_preds)\n\n    model.eval()\n    val_loss = 0\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_dataloader:\n            messages = batch[\"messages\"].to(device)\n            lengths = batch[\"lengths\"]\n            labels = batch[\"labels\"].to(device)\n            num_messages = batch['num_messages']\n            scores = batch['deltas'].to(device)\n            logits = model(messages, num_messages, lengths,scores)\n            loss_ = loss(logits, labels)\n            val_loss += loss_.item()\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            val_preds.extend(preds)\n            val_labels.extend(labels.cpu().numpy())\n\n    val_loss /= len(val_dataloader)\n    val_f1 = f1_score(val_labels, val_preds, average='macro')\n    val_acc = accuracy_score(val_labels,val_preds)\n\n    print(f\"Epoch {epoch+1}:  Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train acc: {train_acc:.4f}  Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val acc: {val_acc:.4f}\")\n    torch.save(model.state_dict(),f\"/kaggle/working/gat_contextlstm_glove_{epoch+1}_recv.pth\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7eDWYJtgSpg","outputId":"bac03c34-5da2-4c9e-fb8c-da347295e309","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:13:56.195056Z","iopub.execute_input":"2025-04-14T17:13:56.195340Z","iopub.status.idle":"2025-04-14T17:16:10.161882Z","shell.execute_reply.started":"2025-04-14T17:13:56.195321Z","shell.execute_reply":"2025-04-14T17:16:10.161029Z"}},"outputs":[{"name":"stdout","text":"Epoch 1:  Train Loss: 0.6942, Train F1: 0.4878, Train acc: 0.9522  Val Loss: 0.6897, Val F1: 0.4906, Val acc: 0.9632\nEpoch 2:  Train Loss: 0.6920, Train F1: 0.4891, Train acc: 0.9509  Val Loss: 0.6894, Val F1: 0.5110, Val acc: 0.9632\nEpoch 3:  Train Loss: 0.6917, Train F1: 0.4942, Train acc: 0.9472  Val Loss: 0.6904, Val F1: 0.4991, Val acc: 0.8926\nEpoch 4:  Train Loss: 0.6919, Train F1: 0.5166, Train acc: 0.9319  Val Loss: 0.6922, Val F1: 0.4306, Val acc: 0.6646\nEpoch 5:  Train Loss: 0.6909, Train F1: 0.2475, Train acc: 0.2773  Val Loss: 0.6945, Val F1: 0.2048, Val acc: 0.2288\nEpoch 6:  Train Loss: 0.6888, Train F1: 0.4334, Train acc: 0.6130  Val Loss: 0.6919, Val F1: 0.4097, Val acc: 0.6019\nEpoch 7:  Train Loss: 0.6857, Train F1: 0.4003, Train acc: 0.5376  Val Loss: 0.6898, Val F1: 0.4578, Val acc: 0.7390\nEpoch 8:  Train Loss: 0.6822, Train F1: 0.4343, Train acc: 0.6091  Val Loss: 0.6951, Val F1: 0.3204, Val acc: 0.4099\nEpoch 9:  Train Loss: 0.6703, Train F1: 0.4468, Train acc: 0.6371  Val Loss: 0.6830, Val F1: 0.4953, Val acc: 0.8299\nEpoch 10:  Train Loss: 0.6547, Train F1: 0.4627, Train acc: 0.6532  Val Loss: 0.7247, Val F1: 0.4146, Val acc: 0.6223\nEpoch 11:  Train Loss: 0.6444, Train F1: 0.5020, Train acc: 0.7571  Val Loss: 0.6895, Val F1: 0.4405, Val acc: 0.6818\nEpoch 12:  Train Loss: 0.6177, Train F1: 0.5247, Train acc: 0.7682  Val Loss: 0.7102, Val F1: 0.4030, Val acc: 0.5705\nEpoch 13:  Train Loss: 0.5768, Train F1: 0.5020, Train acc: 0.7145  Val Loss: 0.6988, Val F1: 0.4422, Val acc: 0.6614\nEpoch 14:  Train Loss: 0.5465, Train F1: 0.5188, Train acc: 0.7366  Val Loss: 0.6969, Val F1: 0.4382, Val acc: 0.6646\nEpoch 15:  Train Loss: 0.5249, Train F1: 0.5489, Train acc: 0.7900  Val Loss: 0.7625, Val F1: 0.4662, Val acc: 0.7312\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfrom sklearn.metrics import accuracy_score\nembedding_dim = 768\nhidden_size = 128\nnum_classes = 2\ngat_dim=50\n\nseed = 100\n\ntorch.manual_seed(seed)\nrandom.seed(seed)\ntorch.cuda.manual_seed_all(seed)\nrandom.seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nmodel = ContextLSTM(embedding_dim, hidden_size, gat_dim, num_classes)\nmodel.load_state_dict(torch.load(\"/kaggle/input/diplomacy-dataset/gat_contextlstm_glove_9_recv (3).pth\"))\nmodel = model.to(device)\n\nclass_weights = torch.tensor([1.0 / 0.05, 1.0 / 0.95], dtype=torch.float).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.003)\nloss = nn.CrossEntropyLoss(weight=class_weights)\n\ntrain_dataset = Deception_dataset_context(train, tokenizer)\nval_dataset = Deception_dataset_context(val, tokenizer)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_context)\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_context)\ntest_dataset = Deception_dataset_context(test, tokenizer)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_context)\n\nmodel.eval()\ntest_preds, test_labels = [], []\nwith torch.no_grad():\n    for batch in test_dataloader:\n        messages = batch[\"messages\"].to(device)\n        lengths = batch[\"lengths\"]\n        labels = batch[\"labels\"].to(device)\n        num_messages = batch['num_messages']\n        scores = batch['deltas'].to(device)\n        logits = model(messages, num_messages,lengths,scores)\n\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n        test_preds.extend(preds)\n        test_labels.extend(labels.cpu().numpy())\n\n\ntest_f1 = f1_score(test_labels, test_preds, average='macro')\ntest_acc = accuracy_score(test_labels,test_preds)\n\nprint(\"test_f1: \",test_f1)\nprint(\"test accuracy: \",test_acc)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzlomltYh7zj","outputId":"6e04ba6a-66f7-4bad-edd5-d0f72f7e33fb","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:31:52.182338Z","iopub.execute_input":"2025-04-14T17:31:52.182641Z","iopub.status.idle":"2025-04-14T17:31:58.547267Z","shell.execute_reply.started":"2025-04-14T17:31:52.182621Z","shell.execute_reply":"2025-04-14T17:31:58.546599Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/461518756.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/input/diplomacy-dataset/gat_contextlstm_glove_9_recv (3).pth\"))\n","output_type":"stream"},{"name":"stdout","text":"test_f1:  0.5246178968250685\ntest accuracy:  0.8202839756592292\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"/kaggle/working/gat_contextlstm_glove_9_recv.pth\")\n","metadata":{"id":"vbqfSpqHa4fa","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:13:02.747760Z","iopub.execute_input":"2025-04-14T17:13:02.748017Z","iopub.status.idle":"2025-04-14T17:13:02.753159Z","shell.execute_reply.started":"2025-04-14T17:13:02.748000Z","shell.execute_reply":"2025-04-14T17:13:02.752513Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/gat_contextlstm_glove_9_recv.pth","text/html":"<a href='/kaggle/working/gat_contextlstm_glove_9_recv.pth' target='_blank'>/kaggle/working/gat_contextlstm_glove_9_recv.pth</a><br>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}